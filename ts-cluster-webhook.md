---

copyright: 
  years: 2014, 2024
lastupdated: "2024-09-30"


keywords: kubernetes, containers

subcollection: containers

content-type: troubleshoot

---

{{site.data.keyword.attribute-definition-list}}





# Why do cluster master operations fail due to a broken webhook?
{: #webhooks_update}
{: support}

[Virtual Private Cloud]{: tag-vpc} [Classic infrastructure]{: tag-classic-inf}

This troubleshooting topic is not for general webhook troubleshooting. See [Debugging webhooks](/docs/containers?topic=containers-ts-webhook-debug) for webhook problems not related to updating the cluster master.
{: note}

During a master operation such as updating your cluster version, the cluster had a broken webhook application.
{: tsSymptoms}

Now, master operations can't complete. You see an error similar to the following:

```sh
Cannot complete cluster master operations because the cluster has a broken webhook application. For more information, see the troubleshooting docs: 'https://ibm.biz/master_webhook'
```
{: screen}


Your cluster has configurable Kubernetes webhook resources, validating or mutating admission webhooks, that can intercept and modify requests from various services in the cluster to the API server in the cluster master.
{: tsCauses}

Because webhooks can change or reject requests, broken webhooks can impact the functionality of the cluster in various ways, such as preventing you from updating the master version or other maintenance operations. For more information, see the [Dynamic Admission Control](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/){: external} in the Kubernetes documentation.

Potential causes for broken webhooks include:
*   The underlying resource that issues the request is missing or unhealthy, such as a Kubernetes service, endpoint, or pod.
*   The webhook is part of an add-on or other plug-in application that did not install correctly or is unhealthy.
*   Your cluster might have a networking connectivity issue that prevents the webhook from communicating with the Kubernetes API server in the cluster master.

Identify and restore the resource that causes the broken webhook.
{: tsResolve}

1. Create a test pod to get an error that identifies the broken webhook. If the test passes, then the failure might have been temporary and can be retried.
    ```sh
    kubectl run webhook-test --image registry.ng.bluemix.net/armada-master/pause:3.10 -n ibm-system
    ```
    {: pre}

    The error message might have the name of the broken webhook. In the following example output, the webhook is `trust.hooks.securityenforcement.admission.cloud.ibm.com`.
    ```sh
    Error from server (InternalError): Internal error occurred: failed calling webhook "trust.hooks.securityenforcementadmission.cloud.ibm.com": Post https://ibmcloud-image-enforcement.ibm-system.svc:443/mutating-pods?timeout=30s: dialtcp 172.21.xxx.xxx:443: connect: connection timed out
    ```
    {: screen}

1. Delete the test pod.

    ```sh
    kubectl delete pod -n ibm-system webhook-test
    ```
    {: pre}

    ```sh
    kubectl label ns ibm-system ibm-cloud.kubernetes.io/webhook-test-at="$(date -u +%FT%H_%M_%SZ)" --overwrite
    ```
    {: pre}



1. Get the name of the broken webhook.
    *   If the error message has a broken webhook, replace `trust.hooks.securityenforcement.admission.cloud.ibm.com` with the broken webhook that you previously identified.
        ```sh
        kubectl get mutatingwebhookconfigurations,validatingwebhookconfigurations -o jsonpath='{.items[?(@.webhooks[*].name=="trust.hooks.securityenforcement.admission.cloud.ibm.com")].metadata.name}{"\n"}'
        ```
        {: pre}

        Example output
        ```sh
        image-admission-config
        ```
        {: pre}

    *   If the error does not have a broken webhook, list all the webhooks in your cluster and check their configurations in the following steps.
        ```sh
        kubectl get mutatingwebhookconfigurations,validatingwebhookconfigurations
        ```
        {: pre}  
            
1. Review the service and location details of the mutating or validating webhook configuration in the `clientConfig` section in the output of the following command. Replace `image-admission-config` with the name that you previously identified. If the webhook exists outside the cluster, contact the cluster owner to check the webhook status.
    ```sh
    kubectl get mutatingwebhookconfiguration image-admission-config -o yaml
    ```
    {: pre}

    ```sh
    kubectl get validatingwebhookconfigurations image-admission-config -o yaml
    ```
    {: pre}

    Example output

    ```sh
      clientConfig:
        caBundle: <redacted>
        service:
            name: <name>
            namespace: <namespace>
            path: /inject
            port: 443
    ```
    {: screen}

1. **Optional**: Back up the webhooks, especially if you don't know how to reinstall the webhook or don't have the required permissions to create webhooks.
    ```sh
    kubectl get mutatingwebhookconfiguration <name> -o yaml > mutatingwebhook-backup.yaml
    ```
    {: pre}

    ```sh
    kubectl get validatingwebhookconfiguration <name> -o yaml > validatingwebhook-backup.yaml
    ```
    {: pre}

1. Check the status of the related service and pods for the webhook.
    1. Check the service **Type**, **Selector**, and **Endpoint** fields.
        ```sh
        kubectl describe service -n <namespace> <service_name>
        ```
        {: pre}

    2. If the service type is **ClusterIP**, check that the Konnectivity pod is in a **Running** status so that the webhook can connect securely to the Kubernetes API in the cluster master. If the pod is not healthy, check the pod events, logs, worker node health, and other components to troubleshoot.

        * Check the Konnectivity agent pods.
            ```sh
            kubectl describe pods -n kube-system -l app=konnectivity-agent
            ```
            {: pre}


    1. If the service does not have an endpoint, check the health of the backing resources, such as a deployment or pod. If the resource is not healthy, check the pod events, logs, worker node health, and other components to troubleshoot. For more information, see [Debugging app deployments](/docs/containers?topic=containers-debug_apps).
        ```sh
        kubectl get all -n my-service-namespace -l <key=value>
        ```
        {: pre}

    1. If the service does not have any backing resources, or if troubleshooting the pods does not resolve the issue, remove the mutating or validating webhook configuration identified earlier.
        ```sh
        kubectl delete validatingwebhookconfiguration NAME
        ```
        {: pre}

        ```sh
        kubectl delete mutatingwebhookconfiguration NAME
        ```
        {: pre}

1. Retry the cluster master operation, such as updating the cluster.
1. If you still see the error, you might have worker node or network connectivity issues.
    *   [Worker node troubleshooting](/docs/containers?topic=containers-debug_worker_nodes).
    *   Make sure that the webhook can connect to the Kubernetes API server in the cluster master. For example, if you use Calico network policies, security groups, or some other type of firewall, set up your [classic](/docs/containers?topic=containers-firewall) or [VPC](/docs/containers?topic=containers-vpc-firewall) cluster with the appropriate access.
    *   If the webhook is managed by an add-on that you installed, uninstall the add-on. Common add-ons that cause webhook issues include the following:
        * [Portieris](/docs/containers?topic=containers-images#portieris-image-sec)
        * [Istio](/docs/containers?topic=containers-istio#istio_uninstall).

1. Re-create the webhook or reinstall the add-on.


1. If the issue persists, contact support. Open a [support case](/docs/get-support?topic=get-support-using-avatar). In the case details, be sure to include any relevant log files, error messages, or command outputs.
